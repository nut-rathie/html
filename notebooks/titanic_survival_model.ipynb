{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic Survival Modeling Notebook\n",
        "\n",
        "Interactively explore the bundled Titanic dataset and train a simple logistic regression model. This notebook mirrors the standalone `analysis/titanic_model.py` script so you can experiment with the pipeline step by step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Requirements\n",
        "\n",
        "This notebook relies only on the Python standard library. If you prefer richer tabular displays, feel free to install `pandas` and adapt the preview cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import math\n",
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Sequence, Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path.cwd().parent / \"data\" / \"titanic.csv\"\n",
        "PREDICTIONS_PATH = Path.cwd().parent / \"analysis\" / \"titanic_predictions.csv\"\n",
        "print(DATA_PATH)\n",
        "print(PREDICTIONS_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_rows(path: Path) -> List[Dict[str, str]]:\n",
        "    with path.open(newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        return list(reader)\n",
        "\n",
        "\n",
        "def infer_fill_value(rows: Iterable[Dict[str, str]], column: str) -> float:\n",
        "    values: List[float] = []\n",
        "    for row in rows:\n",
        "        raw = row[column].strip()\n",
        "        if raw:\n",
        "            try:\n",
        "                values.append(float(raw))\n",
        "            except ValueError:\n",
        "                continue\n",
        "    if not values:\n",
        "        return 0.0\n",
        "    return sum(values) / len(values)\n",
        "\n",
        "\n",
        "def extract_features(rows: Sequence[Dict[str, str]]) -> Tuple[List[List[float]], List[int], List[str]]:\n",
        "    age_default = infer_fill_value(rows, \"Age\")\n",
        "    fare_default = infer_fill_value(rows, \"Fare\")\n",
        "\n",
        "    embarked_categories = [\"C\", \"Q\", \"S\"]\n",
        "    passenger_ids: List[str] = []\n",
        "    features: List[List[float]] = []\n",
        "    targets: List[int] = []\n",
        "\n",
        "    for row in rows:\n",
        "        passenger_ids.append(row[\"PassengerId\"])\n",
        "        targets.append(int(row[\"Survived\"]))\n",
        "\n",
        "        pclass = float(row[\"Pclass\"]) if row[\"Pclass\"] else 0.0\n",
        "        sex = 1.0 if row[\"Sex\"].strip().lower() == \"female\" else 0.0\n",
        "        age = float(row[\"Age\"]) if row[\"Age\"].strip() else age_default\n",
        "        sibsp = float(row[\"SibSp\"]) if row[\"SibSp\"] else 0.0\n",
        "        parch = float(row[\"Parch\"]) if row[\"Parch\"] else 0.0\n",
        "        fare = float(row[\"Fare\"]) if row[\"Fare\"].strip() else fare_default\n",
        "\n",
        "        embarked_one_hot = [0.0, 0.0, 0.0]\n",
        "        embark_value = row[\"Embarked\"].strip().upper()\n",
        "        if embark_value in embarked_categories:\n",
        "            embarked_one_hot[embarked_categories.index(embark_value)] = 1.0\n",
        "\n",
        "        feature_row = [\n",
        "            1.0,\n",
        "            pclass,\n",
        "            sex,\n",
        "            age,\n",
        "            sibsp,\n",
        "            parch,\n",
        "            fare,\n",
        "            *embarked_one_hot,\n",
        "        ]\n",
        "        features.append(feature_row)\n",
        "\n",
        "    return features, targets, passenger_ids\n",
        "\n",
        "\n",
        "def standardize_features(features: List[List[float]]) -> List[List[float]]:\n",
        "    if not features:\n",
        "        return features\n",
        "\n",
        "    num_features = len(features[0])\n",
        "\n",
        "    for index in range(num_features):\n",
        "        if index == 0:\n",
        "            continue\n",
        "        column = [row[index] for row in features]\n",
        "        mean = sum(column) / len(column)\n",
        "        variance = sum((value - mean) ** 2 for value in column) / len(column)\n",
        "        std = math.sqrt(variance) if variance > 0 else 1.0\n",
        "\n",
        "        for row in features:\n",
        "            row[index] = (row[index] - mean) / std\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sigmoid(value: float) -> float:\n",
        "    if value >= 0:\n",
        "        z = math.exp(-value)\n",
        "        return 1.0 / (1.0 + z)\n",
        "    z = math.exp(value)\n",
        "    return z / (1.0 + z)\n",
        "\n",
        "\n",
        "def dot_product(a: Sequence[float], b: Sequence[float]) -> float:\n",
        "    return sum(x * y for x, y in zip(a, b))\n",
        "\n",
        "\n",
        "def train_logistic_regression(\n",
        "    features: List[List[float]],\n",
        "    targets: Sequence[int],\n",
        "    learning_rate: float = 0.1,\n",
        "    epochs: int = 800,\n",
        ") -> List[float]:\n",
        "    if not features:\n",
        "        return []\n",
        "\n",
        "    weights = [0.0 for _ in range(len(features[0]))]\n",
        "    n = len(features)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        gradients = [0.0 for _ in weights]\n",
        "        for row, target in zip(features, targets):\n",
        "            prediction = sigmoid(dot_product(weights, row))\n",
        "            error = prediction - target\n",
        "            for index, value in enumerate(row):\n",
        "                gradients[index] += error * value\n",
        "\n",
        "        for index in range(len(weights)):\n",
        "            weights[index] -= (learning_rate / n) * gradients[index]\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def predict_probabilities(features: Iterable[Sequence[float]], weights: Sequence[float]) -> List[float]:\n",
        "    probabilities: List[float] = []\n",
        "    for row in features:\n",
        "        probabilities.append(sigmoid(dot_product(weights, row)))\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def evaluate(predictions: Sequence[float], targets: Sequence[int]) -> Tuple[float, float]:\n",
        "    total = len(targets)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    correct = 0\n",
        "    log_loss = 0.0\n",
        "    for probability, target in zip(predictions, targets):\n",
        "        predicted_label = 1 if probability >= 0.5 else 0\n",
        "        if predicted_label == target:\n",
        "            correct += 1\n",
        "\n",
        "        prob = min(max(probability, 1e-12), 1 - 1e-12)\n",
        "        if target == 1:\n",
        "            log_loss -= math.log(prob)\n",
        "        else:\n",
        "            log_loss -= math.log(1 - prob)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    average_log_loss = log_loss / total\n",
        "    return accuracy, average_log_loss\n",
        "\n",
        "\n",
        "def write_predictions(\n",
        "    passenger_ids: Sequence[str],\n",
        "    probabilities: Sequence[float],\n",
        "    path: Path,\n",
        ") -> None:\n",
        "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"PassengerId\", \"PredictedProbability\", \"PredictedSurvived\"])\n",
        "        for passenger_id, probability in zip(passenger_ids, probabilities):\n",
        "            writer.writerow([\n",
        "                passenger_id,\n",
        "                f\"{probability:.6f}\",\n",
        "                1 if probability >= 0.5 else 0,\n",
        "            ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset\n",
        "\n",
        "Read the Titanic manifest and inspect a few sample rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = load_rows(DATA_PATH)\n",
        "len(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in islice(rows, 5):\n",
        "    print({key: row[key] for key in (\"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Fare\")})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare features\n",
        "\n",
        "Convert the raw CSV rows into numerical features suitable for logistic regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features, targets, passenger_ids = extract_features(rows)\n",
        "standardized_features = standardize_features(features)\n",
        "len(features), len(targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate the model\n",
        "\n",
        "Fit a basic logistic regression classifier and review its performance on the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights = train_logistic_regression(standardized_features, targets)\n",
        "probabilities = predict_probabilities(standardized_features, weights)\n",
        "accuracy, average_log_loss = evaluate(probabilities, targets)\n",
        "print(f\"Training accuracy: {accuracy:.3f}\")\n",
        "print(f\"Average log loss: {average_log_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect predictions\n",
        "\n",
        "View the first few probability estimates alongside their passenger identifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for passenger_id, probability in islice(zip(passenger_ids, probabilities), 5):\n",
        "    label = \"survived\" if probability >= 0.5 else \"did not survive\"\n",
        "    print(passenger_id, f\"{probability:.3f}\", f\"({label})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save results\n",
        "\n",
        "Persist the predictions to `analysis/titanic_predictions.csv` so they match the CLI workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "write_predictions(passenger_ids, probabilities, PREDICTIONS_PATH)\n",
        "PREDICTIONS_PATH\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
